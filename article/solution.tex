\section{Описание решения}
В этой главе рассмотрен весь алгоритм распознавания речевых команд. Для удобства понимания названия параграфов расположены в том же порядке, что и этапы в самом алгоритме.

\subsection{Предобработка}
Каждая команда записана в звуковой wav файл. В каждом файле - набор амплитудных значений, которые были получены в результате записи команды дикторами. 

\subsubsection{Нормализация сигнала}
Сначала проводится нормализация амплитуд. Каждое значение амплитуды приводится к такому значению, чтобы максимум среди всех амплитуд звуковой дорожки был равен единице по формуле:
\begin{equation}
	\overline{x}_i=\dfrac{x_i}{\max_{j} |x_j|},~i=\overline{0, p-1},~j \in [0, p-1],
\end{equation}
где $x$ - значение амплитуды, $\overline{x}$ - новое значение амплитуды, $p$ - количество амплитудных значений в звуковой дорожке.

Таким образом, все значения амплитуд принимают значения в диапазоне $[0,1]$.

\subsubsection{Удаление постоянной составляющей}
Постоянная составляющая (DC-offset) - это смещение амплитуды сигнала на некоторую постоянную величину. Она возникает в цифровом сигнале, полученном с АЦП,  из-за разницы напряжения между звуковой картой и устройством ввода. Данный эффект является помехой, от которой нужно избавиться. Для этого необходимо вычесть из каждого значения амплитуды среднее арифметическое всех значений амплитуд по формуле:
\begin{equation}
\overline{x}_i=x_i - \sum_{j=0}^{p-1} x_j,~i=\overline{0, p-1},
\end{equation}
где $x$ - значение амплитуды полученное на этапе нормализации, $\overline{x}$ - новое значение амплитуды, $p$ - количество амплитудных значений в звуковой дорожке.

Все $\overline{x}_i$ переобозначаются как $x_i$.

\subsubsection{Выделение начальной и конечной точек слова}
Каждая звуковая дорожка содержит в себе помимо фрагментов звукового сигнала (команды) ещё и фрагменты тишины. Очень важно отделить звуковой сигнал от фрагментов тишины, так как именно он несёт в себе всю информацию о команде. 

Для того, чтобы выделить звуковой сигнал и <<обрезать>> тишину в начале и в конце записи, используется алгоритм, описанный в статье \cite{SignalPreprocessing}. 
Каждая звуковая дорожка разбивается на фреймы - наборы амплитуд, каждый длительностью 20 мс. Начала фреймов расположены с периодичностью 10 мс. Таким образом, фреймы пересекаются между собой. Это обеспечивает целостность обработки звукового сигнала, позволяя обрабатывать важные фонемообразующие особенности.

Затем для каждого фрейма вычисляется мгновенная энергия:
\begin{equation}
E_k = \sum_{m=0}^{N-1} x_{k_m}^2,~k=\overline{0,z-1},
\end{equation}
где $z$ - количество фреймов для конкретной звуковой записи, $N$ - длина одного фрейма (количество амплитуд в одном фрейме).

Вычисление функции мгновенной энергии имеет значительный недостаток. У неё велика чувствительность к относительно большим значениям амплитуды из-за возведения их во вторую степень. Это ведёт к искажению соотношений амплитудных значений звукового сигнала друг к другу. Поэтому функция мгновенной энергии переопределяется как:
\begin{equation}
\label{eq:instant_energy}
E_k = \sum_{m=0}^{N-1} |x_{k_m}|,~k=\overline{0,z-1},
\end{equation}
где $z$ - количество фреймов для конкретной звуковой записи, $N$ - длина одного фрейма.

После того, как посчитаны мгновенные энергии для каждого фрейма, вычисляются нижнее и верхнее пороговые значения:
\begin{equation}
\begin{aligned}
& I_1 = 0.03 \cdot (MX - MN) + MN \\
& I_2 = 4 \cdot MN \\
& ITL = min(I_1,~I_2)\\
& ITU = 10 \cdot ITL,
\end{aligned}
\end{equation}
где $MN$, $MX$ - минимум и максимум мгновенной энергии среди всех фреймов соответственно, $ITL$, $ITU$ - нижнее и верхнее пороговое значение.

Происходит поиск фрейма, с которого начинается слово, начиная с самого первого фрейма. Фрейм, в котором значение мгновенной энергии превышает $ITL$, предварительно помечается как начало слова. Затем, начиная с этого помеченного фрейма, происходит поиск фрейма, в котором значение мгновенной энергии превышает $ITU$. Если значение мгновенной энергии для какого-то фрейма во время последнего поиска меньше $ITL$, то этот фрейм становится предварительным началом слова. 

Аналогично происходит поиск конца слова в звуковой дорожке, но поиск по фреймам происходит не с начала сигнала, а с конца.

После этого этапа имеются два предварительно помеченных фрейма $m_{begin}, m_{end}$  - начало и конец слова в звуковом файле соответственно.

Функция мгновенной энергии, определённая формулой \eqref{eq:instant_energy}, хорошо справляется с отделением звонких звуков от тишины. Но глухие она отделяет плохо. Поэтому используется вторая характеристика для доопределения начала и конца слова - число переходов через ноль. Это количество таких случаев, когда соседние значения амплитуд имеют противоположные знаки. Определяется формулой:
\begin{equation}
	Z_k = \dfrac{1}{2} \sum_{m=1}^{N-1} |sgn(x_{k_{m-1}}) - sgn(x_{k_m})|,~k=\overline{0,z-1},
\end{equation}
где $z$ - количество фреймов для конкретной звуковой записи, $N$ - длина одного фрейма.

Подразумевается, что первые 100 мс звуковой записи - это тишина, и речь начинается позднее.

Вычисляется среднее значение переходов через ноль в течение первых 100 мс \eqref{eq:izc} и среднее квадратическое отклонение количества переходов через ноль в течение первых 100 мс \eqref{eq:deviation}:
\begin{align}
	\label{eq:izc}
	&IZC = \dfrac{1}{z} \sum_{k=0}^{z-1} Z_k \\
	\label{eq:deviation}
	&\sigma_{IZC} = \sqrt{\dfrac{1}{z} \sum_{k=0}^{z-1} (Z_k - IZC)^2},
\end{align}
где $z$ - количество фреймов для конкретной звуковой записи.
Затем вычисляется значение пороговой функции числа переходов через ноль по формуле:
\begin{equation}
	IZCT = min(IF,~IZC + 2 \sigma_{IZC}),
\end{equation}
где $IF$ - фиксированное количество переходов через ноль. В данном случае оно составляет 25 пересечений за 10 мс, то есть $IF=2.5$. 

Далее происходит уточнение точек начала и конца слова в звуковой дорожке. Начиная от фрейма $m_{begin}$ влево происходит поиск фреймов, у которых число переходов через ноль выше порогового значения. Поиск происходит на расстоянии 25 фреймов, так как производится уточнение границ слова. Если пороговое значение было превышено 3 или более раз, то фрейм, где это произошло впервые, помечается как начало слова и обозначается как $r_{begin}$. Если пороговое значение было превышено менее 3-х раз, то метка $m_{begin}$ переобозначается как $r_{begin}$.

Аналогично от фрейма $m_{end}$ происходит поиск вправо для уточнения точки конца слова, которая обозначается как $r_{end}$.

В результате уточнения точек начала и конца слова имеются 2 помеченных фрейма - $r_{begin}, r_{end}$. Сигнал обрезается, и в нем остаётся только речевая команда в виде набора фреймов $[r_{begin}, ... , r_{end}]$. Количество получившихся фреймов обозначается как $u$, а сами фреймы перенумеруются следующим образом: $[r_0, ... , r_{u-1}]$.

\subsection{Выделение речевых признаков}
\subsubsection{Общая схема алгоритма}
Для того, чтобы выделить речевые признаки, используется алгоритм мел-частотных кепстральных коэффициентов \cite{MFCC} (далее - MFCC). Он является одним из стандартных подходов к решению поставленной задачи. Состоит MFCC из нескольких шагов:
\begin{enumerate}
	\item Для каждого звукового сигнала проделать шаги:
	\begin{enumerate}
		\item Разбить сигнал на фреймы.
		\item Для каждого фрейма проделать шаги:
		\begin{enumerate}
			\item Получить спектр мощности сигнала.
			\item Составить набор мел-фильтров.
			\item Применить мел-фильтры к спектру мощности сигнала.
			\item Прологарифмировать результат, полученный на предыдущем шаге.
			\item Применить дискретное косинусное преобразование к результату предыдущего шага.
		\end{enumerate}
		\item Объединить MFCC векторы коэффициентов в матрицу.
	\end{enumerate}

	\item Привести матрицы коэффициентов MFCC каждой звуковой дорожки к одной размерности. Для этого дополнить их нулями слева до необходимой длины. Унифицированная размерность матриц выбирается как максимальная среди всех.
\end{enumerate}

\subsubsection{Разбиение сигнала на фреймы}
Амплитуда сигнала постоянно изменяется. Для упрощения предполагается, что в течение коротких промежутков времени амплитуда сигнала является статистически постоянной. Поэтому сигнал разделяется на фреймы длительностью 20-40 мс (в рамках данной работы - 20 мс). Если взять длину фрейма короче этой длительности, то количество амплитудных значений в каждом фрейме будет недостаточным для построения достоверной спектральной оценки для всего сигнала. Если взять длину фрейма длиннее чем 20-40 мс, то сигнал уже не будет статистически постоянным на этом промежутке времени.

Так как на выходе алгоритма выделения начальной и конечной точек слова получается набор фреймов необходимой длительности, то шаг разбиения сигнала на фреймы опускается.

\subsubsection{Получение спектра мощности сигнала}
Следующий шаг - вычисление спектра мощности сигнала для каждого фрейма. Этот шаг обоснован особенностью восприятия звука человеком. В человеческом ухе улитка вибрирует в разных зонах в зависимости от частоты входного звукового сигнала. И в зависимости от того, какая зона (волосковых клеток) в улитке вибрирует, соответствующие нервы информируют мозг об определённых частотах, которые содержатся в звуковом сигнале. Анализ спектра мощности сигнала в компьютерном представлении позволяет провести аналогичное разложение сигнала на частоты, которые в нём присутствуют.

Спектр мощности сигнала $S_{k_m}$ в $k$-ом фрейме - результат дискретного преобразования Фурье:
\begin{equation}
	S_{k_m} = \sum_{n=0}^{N-1} x_{k_n} \cdot e^{\dfrac{-2\pi i}{N}mn},~m=\overline{0,N-1},~k=\overline{0,u-1},
\end{equation}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $N$ - длина одного фрейма, $i$ - мнимая единица.

\subsubsection{Расчёт энергий спектра мощности при помощи мел-фильтров}
Спектр мощности звукового сигнала содержит в себе излишнюю информацию, которая не требуется для алгоритма распознавания речи. В частности, улитка человеческого уха не может отличить две близкие по значениям частоты во входном звуковом сигнале. Данный эффект более выражен для более высоких частотных значений. В связи с этим, спектр мощности делится на определённые частотные интервалы, для каждого из которых считается энергия. Это производится при помощи мел-фильтров, выраженных в виде оконной функции. Частотный интервал первого фильтра - очень узкий. Он даёт представление о существующем количестве энергии около частоты 0 Гц. Более высоким частотным значениям соответствуют более широкие интервалы фильтров. Это объясняется тем, что для более высоких частотных значений разница между соседними становится все менее значимой. Мел-шкала позволяет точно рассчитать ширины частотных интервалов для фильтров.

Мел - психофизическая единица высоты звука. Используемые формулы для перевода частоты из шкалы Герц в шкалу 
мел и обратно описаны в книге \cite{Mel} на стр. 150:
\begin{align}
	\label{eq:mel}
	&mel(hz) = 1127 \cdot ln(1 + \dfrac{hz}{700})\\
	\label{eq:hz}
	&hz(mel) = 700 \cdot (e^{\dfrac{mel}{1127}}-1)
\end{align}

На рисунке \ref{fig:hz_mel} изображено сравнение шкал мел и Гц.

\begin{figure}[H]
	\[\includegraphics[scale=0.3]{hz_mel.png}\]
	\caption{Сравнение шкал мел и Гц}
	\label{fig:hz_mel}
\end{figure}

Составляются треугольные мел-фильтры в виде оконной функции:
\begin{equation}
	H(v,~b)=
	\begin{cases}
		0, 										     & \Phantom \phantom{-}    b < f(v)\\
		\dfrac{b-f(v)}{f(v+1)-f(v)},   & \Phantom f(v) \leq b < f(v+1)\\
		\dfrac{f(v+2)-b}{f(v+2)-f(v+1)}, & \Phantom f(v+1) \leq b < f(v+2)\\
		0,                                           & \Phantom \phantom{-}   f(v+2) < b\\
	\end{cases}
,
\end{equation}
для которой $f$ определяется как:
\begin{equation}
	f(a)=\dfrac{N}{w} hz(mel(f_{min})+a \dfrac{mel(f_{max})-mel(f_{min})}{Q+1}),
\end{equation}
где $mel$ и $hz$ - функции, определённые формулами \eqref{eq:mel} и \eqref{eq:hz} соответственно, $w$ - частота дискретизации звуковой дорожки, $f_{min},~f_{max}$ - нижний и верхний пороги частотного диапазона соответственно, $N$ - длина одного фрейма, $Q$ - количество мел-фильтров. Параметр Q рекомендовано выбирать в диапазоне 20-40 (в рамках данной работы - 26) \cite{MFCC}.

На рисунке \ref{fig:filters} в качестве примера изображена оконная функция для звуковой дорожки с параметрами $w=44100$ Гц, $f_{min}=0$ Гц, $f_{max}=22050$ Гц, $N=1024$, $Q=13$.

\begin{figure}[H]
	\[\includegraphics[scale=0.3]{filters.png}\]
	\caption{Оконная функция}
	\label{fig:filters}
\end{figure}

Считаются энергии спектра мощности при помощи мел-фильтров:
\begin{equation}
	W_{k_q} = \sum_{m=0}^{N-1} |S_{k_m}|^2 \cdot H(q,m),~q=\overline{0,Q-1},~k=\overline{0,u-1},
\end{equation}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $Q$ - выбранное количество мел-фильтров.

\subsubsection{Логарифмирование энергий спектра мощности сигнала}
После получения энергий спектра мощности сигнала при помощи мел-фильтров, происходит логарифмирование последних. Обосновано это тем, что громкость речи нелинейно зависит от количества содержащейся в ней энергии. Чтобы удвоить громкость произносимой речи, человеку необходимо приложить в 8 раз больше энергии. Это означает, что фрагменты речи, имеющие большое различие в соответствующих им значениях энергий могут иметь примерно одинаковую громкость, если речь начинается громко. Операция логарифмирования производит корреляцию между громкостью и энергией звукового сигнала более похожей на человеческую.

Логарифмирование энергий спектра мощности, полученных при помощи мел-фильтров:
\begin{equation}
	V_{k_q} = ln(W_{k_q}),~q=\overline{0,Q-1},~k=\overline{0,u-1},
\end{equation}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $Q$ - выбранное количество мел-фильтров.

\subsubsection{Получение кепстральных коэффициентов}
Последним шагом является вычисление дискретного косинусного преобразования от прологарифмированных энергий спектра мощности сигнала. Так как мел-фильтры пересекаются между собой, значения энергий, полученных при помощи соответствующих мел-фильтров, сильно коррелируют друг с другом. Дискретное косинусное преобразование декоррелирует их. Коэффициенты этого преобразования называются кепстральными, так как все вместе они образуют кепстр сигнала. В качестве результата берутся первые 12-13 (в рамках данной работы - 12) этих коэффициентов. Исследователями в этой области эмпирическим путём показано, что использование последующих коэффициентов ухудшает точность распознавания в сравнении с использованием первых 12-13.

Стоит отметить, что кепстр определяется в виде прямого преобразования Фурье от логарифма спектра мощности сигнала:
\begin{equation}
	C(x(t))=F(ln[F(x[t])]),
\end{equation}
где $x(t)$ - входной сигнал, $F$ - функция прямого преобразования Фурье. В книге \cite{CeptrumExplanation} можно найти описание кепстра на стр. 367-380. 

Однако в алгоритме MFCC используется дискретное косинусное преобразование как частный случай прямого преобразования Фурье по вышеописанным причинам.

Получение итоговых коэффициентов MFCC происходит по формуле:
\begin{equation}
	c_{k_n} = \sum_{m=0}^{Q-1} V_{k_m} cos(\dfrac{\pi}{Q} (m+\dfrac{1}{2})n),~n=\overline{0,d-1},d \le Q,~k=\overline{0,u-1},
\end{equation}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $Q$ - выбранное количество мел-фильтров, $d$ - выбранное количество коэффициентов MFCC (в рамках данной работы - 12) \cite{MFCC}, $\pi$ - математическая константа.

Таким образом, строится матрица для каждой звуковой дорожки следующего вида:

\begin{equation*}
	C_{u \times d} = \left(
	\begin{array}{cccc}
		c_{00} & c_{01} & \ldots & c_{0(d-1)}\\
		c_{10} &  c_{11} & \ldots & c_{1(d-1)}\\
		\vdots & \vdots & \ddots & \vdots\\
		c_{(u-1)0} & c_{(u-1)1} & \ldots & c_{(u-1)(d-1)}
	\end{array}
	\right)
\end{equation*}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $d$ - выбранное количество коэффициентов MFCC.

\label{par:unify_coeffs}
\subsubsection{Приведение данных к одной размерности}
Среди всех значений $u$ существует максимальное $u_{max}$. Для каждой матрицы, соответствующей определённой звуковой дорожке, проделаем следующее:
\begin{itemize}[leftmargin=2cm]
	\item если $u < u_{max}$, то соответствующая матрица $C$ дополняется нулями слева:
	\begin{equation*}
		C_{u_{max} \times d} = \left(
		\begin{array}{ccccccc}
			0 & \ldots & 0 & c_{00} & c_{01} & \ldots & c_{0(d-1)}\\
			0 & \ldots & 0 & c_{10} &  c_{11} & \ldots & c_{1(d-1)}\\
			\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
			0 & \ldots & 0 & c_{(u-1)0} & c_{(u-1)1} & \ldots & c_{(u-1)(d-1)}
		\end{array}
		\right)
	\end{equation*}
	\item если $u = u_{max}$, то матрица $C$ остаётся без изменений.
\end{itemize}

Таким образом, все матрицы приводятся к одной размерности $u_{max} \times d$.

\subsection{Распознавание речевых команд}
Для распознавания речевых команд используются технологии нейронных сетей. В данной работе рассматриваются два типа нейронных сетей: многослойный персептрон и свёрточная сеть. Производится сравнение производительности этих двух типов при различных параметрах обучения.
\subsubsection{Многослойный персептрон}
Этот вид нейронной сети описан в третьей части книги Фрэнка Розенблатта \cite{PerceptronBook}, который первый предложил модель персептрона. Она состоит из входного слоя, полносвязных слоев и выходного слоя.
\subsubsection{Свёрточная нейронная сеть}
Данный тип нейронной сети был предложен Яном ЛеКуном \cite{CNN}. Это многослойная сеть, позволяющая обеспечить устойчивость распознавания к инвариантным изменениям данных за счёт общих весов и локального рецептивного поля.

Входной слой сети состоит из одной плоскости. Его размерность совпадает с размерностью входных данных. 

Последующие слои - свёрточные. Каждый свёрточный слой состоит из нескольких плоскостей нейронов, которые известны как карты признаков. Каждый нейрон в свёрточном слое соединён с небольшой областью предыдущего слоя. В этом заключается принцип локального рецептивного поля.

После каждого свёрточного слоя, который получил локальные признаки, следует пулинг слой. Его задача состоит в понижении размерности данных.

После всех свёрточных слоёв следует выпрямляющий слой, который преобразует данные к вектору. 

Далее следуют полносвязные слои. Иногда добавляется дропаут слой, который отключает некоторые случайные нейроны в процессе обучения. Это позволяет бороться с переобучением сети.

Выходной слой имеет размерность требуемых выходных данных. 

\subsubsection{Входные и выходные данные модели}
Входной тензор модели - матрица MFCC коэффициентов для соответствующей команды. Его размерность - $u_{max} \times d$. Для составленного датасета $u_{max}=400, d=13$.

Выходной тензор имеет размерность $g \times 1$, где $g$ - количество возможных команд для распознавания. Для составленного датасета $g=11$. Скалярная метка распознанной команды соответствует индексу максимального элемента выходного тензора. Индексация в выходном тензоре начинается с 0.
\subsubsection{Архитектура модели нейронной сети}
Архитектура свёрточной нейронной сети приведена на рисунке \ref{fig:cnn_model}. Как видно из рисунка, сеть включает в себя:
\begin{itemize}[leftmargin=2cm]
	\item Входной слой InputLayer (размерность входных данных - $400 \times 13$)
	\item Слой свёртки Conv2D (32 нейрона, размерность ядра сверки - $5 \times 5$, функция активации - ReLu)
	\item Слой пулинга AveragePooling2D (размерность пула - $2 \times 2$)
	\item Слой свёртки Conv2D (64 нейрона, размерность ядра сверки - $5 \times 5$, функция активации - ReLu)
	\item Слой пулинга AveragePooling2D (размерность пула - $2 \times 2$)
	\item Выпрямляющий слой Flatten
	\item Полносвязный слой Dense (128 нейронов, функция активации - ReLu)
	\item Слой дропаута Dropout (процент исключения случайных нейронов от общего числа в слое - 30\%)
	\item Выходной полносвязный слой Dense (11 нейронов, функция активации - Softmax)
\end{itemize}

\begin{figure}[H]
	\[\includegraphics[scale=0.4]{cnn_model.png}\]
	\caption{Структура свёрточной нейронной сети}
	\label{fig:cnn_model}
\end{figure}

Архитектура свёрточной нейронной сети приведена на рисунке \ref{fig:cnn_model}. Как видно из рисунка, сеть включает в себя:
\begin{itemize}[leftmargin=2cm]
	\item Входной слой InputLayer (размерность входных данных - $400 \times 13$)
	\item Выпрямляющий слой Flatten
	\item Полносвязный слой Dense (256 нейронов, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Полносвязный слой Dense (128 нейронов, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Полносвязный слой Dense (128 нейронов, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Полносвязный слой Dense (64 нейрона, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Выходной полносвязный слой Dense(11 нейронов, функция активации - Softmax)
\end{itemize}

\begin{figure}[H]
	\[\includegraphics[scale=0.4]{mlp_model.png}\]
	\caption{Структура многослойного персептрона}
	\label{fig:cnn_model}
\end{figure}

\subsection{Программная реализация}
Построение программного интерфейса для блоков предобработки и распознавания команд было произведено при помощи языка программирования Python 3.8.

Блок предобработки был реализован при помощи библиотек numpy, scipy. 

Блок распознавания был реализован при помощи библиотек jupyterlab, Keras, numpy. Для анализа результатов обучения нейронных сетей были использованы библиотеки pandas, matplotlib, seaborn.

Программный комплекс разделен на две основные части:
\begin{itemize}[leftmargin=2cm]
	\item Корневой файл <<preprocessing.py>>, в котором реализована предобработка. Весь вспомогательный функционал вынесен в отдельный модуль <<helpers>>.
	\item Корневой файл <<boxy.ipynb>>, в котором реализовано обучение и тестирование нейронных сетей. Весь вспомогательный функционал также вынесен в отдельный модуль <<helpers>>.
\end{itemize}

В корневой директории <<recorded\_audio>> хранятся записанные шестью разными дикторами звуковые файлы, содержащие команды. Каждый файл содержит в своём названии скалярную метку речевой команды, которая в нём записана.

В корневую директорию <<data>>, в процессе предобработки файлов из директории <<recorded\_audio>>, сохраняются файлы:
\begin{itemize}[leftmargin=2cm]
	\item <<speaker\{$i$\}\_data.npy>>\footnotemark 
	\item <<speaker\{$i$\}\_labels.npy>>\footnotemark[\value{footnote}],
\end{itemize}
где  $i=\overline{1,6}$ - номер диктора. Их описание приводится в разделе \ref{par:saving_preprocessed_data}.

В корневой директории <<logs>> в процессе предобработки данных создаётся файл <<log.log>>, в который записывается краткий отчёт об обработке каждого звукового файла.

В корневой директории <<model\_checkpoints>> в процессе обучения нейронной сети создаются дочерние директории <<experiment\{$i$\}>>\footnotemark[\value{footnote}], $i=\overline{1,3}$ - номер эксперимента. В дочерних директориях создаются свои дочерние директории <<{nn\_type}>>. Параметр nn\_type принимает  значения <<cnn>> при обучении свёрточной сети и <<mlp>> при обучении многослойного персептрона. В директорию <<{nn\_type}>> сохраняются веса сети с наилучшей точностью распознавания на валидационных данных.

\footnotetext[1]{Фигурные скобки не являются частью имени файла или директории.}

Для создания датасета было разработано веб-приложение при помощи языков Javascript, HTML, CSS и фреймворка NodeJS. Оно позволяет записывать речевые команды и сохранять их в файлы в нужном для программы предобработки wav-формате с названиями, содержащими соответствующие скалярные метки. В распространённых программах записи звука много времени тратится на сохранение с правильными параметрами звуковой дорожки. Сравнительно с ними созданное веб-приложение позволило в короткие сроки записать необходимое количество дикторов и сократило временные затраты на запись образцов речи в несколько раз. Код веб-приложения доступен в приложении.

\subsection{Описание датасета}
Составленный датасет состоит из 11 команд, записанных шестью дикторами. Каждый диктор работал с командами : <<back>>, <<down>>, <<menu>>, <<off>>, <<on>>, <<open>>, <<play>>, <<power>>, <<stop>>, <<up>>, <<volume>>. В таблице \ref{table:dataset} указаны типы голосов дикторов и данные о количестве записанных команд.
\begin{table}[H]
\begin{tabular}[c]{ | p{1.8cm} | p{2.5cm} | p{6cm} | p{4cm} | }
	\hline
	Диктор & Тип голоса & Кол-во звук. дорожек на каждую команду & Сумм. кол-во звук. дорожек  \\ \hline
	speaker1 & Мужской & 50 & 550 \\
	speaker2 & Мужской & 40 & 440 \\
	speaker3 & Мужской & 40 & 440 \\
	speaker4 & Мужской & 40 & 440 \\
	speaker5 & Мужской & 50 & 550 \\
	speaker6 & Женский & 50 & 550 \\ \hline
	
\end{tabular}
\caption{\label{table:dataset}Типы дикторов и данные о количестве записанных команд}
\end{table}

Датасет предварительно разделяется на тренировочную и тестовую части. На тренировочную часть отводится 70\% данных каждого диктора, на тестовую часть - 30\%.

\label{par:saving_preprocessed_data}
Матрицы коэффициентов MFCC, полученных на этапе \ref{par:unify_coeffs} объединяются в массив и записываются в файл <<speaker\{$i$\}\_data.npy>>\footnotemark[\value{footnote}] \space в виде numpy массива. В файл <<speaker\{$i$\}\_labels.npy>>\footnotemark[\value{footnote}] записываются скалярные метки команд в виде массива, в том же порядке, что и соответствующие им матрицы коэффициентов MFCC. Здесь $i=\overline{1,6}$  - номер диктора. Соответствие произнесённых диктором команд и их скалярных меток представлено в таблице \ref{table:commands}.

\footnotetext[1]{Фигурные скобки не являются частью имени файла или директории.}

\begin{table}[H]
	\small
	\begin{tabular}[c]{ | l | l | l | l | l | l | l | l | l | l | l | l |}
		\hline
		Команда	 &  back & down & menu & off & on & open & play & power & stop & up & volume \\ \hline
		Скалярная метка & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
		
	\end{tabular}
	\caption{\label{table:commands}Соответствие произнесённых диктором команд и их скалярных меток}
\end{table}