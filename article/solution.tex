\section{Описание решения}
В этой главе рассмотрен весь алгоритм распознавания речевых команд. Для удобства понимания названия параграфов расположены в том же порядке, что и этапы в самом алгоритме.

\subsection{Предобработка}
Каждая команда записана в звуковой wav файл. В каждом файле - набор амплитудных значений, которые были получены в результате записи команды дикторами. 

\subsubsection{Нормализация сигнала}
Сначала проводится нормализация амплитуд. Каждое значение амплитуды приводится к такому значению, чтобы максимум среди всех амплитуд звуковой дорожки был равен единице по формуле:
\begin{equation}
	\overline{x}_i=\dfrac{x_i}{\max_{j} |x_j|},~i=\overline{0, p-1},~j \in [0, p-1],
\end{equation}
где $x$ - значение амплитуды, $\overline{x}$ - новое значение амплитуды, $p$ - количество амплитудных значений в звуковой дорожке.

Таким образом, все значения амплитуд принимают значения в диапазоне $[0,1]$.

\subsubsection{Удаление постоянной составляющей}
Постоянная составляющая (DC-offset) - это смещение амплитуды сигнала на некоторую постоянную величину. Она возникает в цифровом сигнале, полученном с АЦП,  из-за разницы напряжения между звуковой картой и устройством ввода. Данный эффект является помехой, от которой нужно избавиться. Для этого необходимо вычесть из каждого значения амплитуды среднее арифметическое всех значений амплитуд по формуле:
\begin{equation}
\overline{x}_i=x_i - \sum_{j=0}^{p-1} x_j,~i=\overline{0, p-1},
\end{equation}
где $x$ - значение амплитуды полученное на этапе нормализации, $\overline{x}$ - новое значение амплитуды, $p$ - количество амплитудных значений в звуковой дорожке.

Все $\overline{x}_i$ переобозначаются как $x_i$.

\subsubsection{Выделение начальной и конечной точек слова}
Каждая звуковая дорожка содержит в себе помимо фрагментов звукового сигнала (команды) ещё и фрагменты тишины. Очень важно отделить звуковой сигнал от фрагментов тишины, так как именно он несёт в себе всю информацию о команде. 

Для того, чтобы выделить звуковой сигнал и <<обрезать>> тишину в начале и в конце записи, используется алгоритм, описанный в статье \cite{SignalPreprocessing}. 
Каждая звуковая дорожка разбивается на фреймы - наборы амплитуд, каждый длительностью 20 мс. Начала фреймов расположены с периодичностью 10 мс. Таким образом, фреймы пересекаются между собой. Это обеспечивает целостность обработки звукового сигнала, позволяя обрабатывать важные фонемообразующие особенности.

Затем для каждого фрейма вычисляется мгновенная энергия:
\begin{equation}
E_k = \sum_{m=0}^{N-1} x_{k_m}^2,~k=\overline{0,z-1},
\end{equation}
где $z$ - количество фреймов для конкретной звуковой записи, $N$ - длина одного фрейма (количество амплитуд в одном фрейме).

Вычисление функции мгновенной энергии имеет значительный недостаток. У неё велика чувствительность к относительно большим значениям амплитуды из-за возведения их во вторую степень. Это ведёт к искажению соотношений амплитудных значений звукового сигнала друг к другу. Поэтому функция мгновенной энергии переопределяется как:
\begin{equation}
\label{eq:instant_energy}
E_k = \sum_{m=0}^{N-1} |x_{k_m}|,~k=\overline{0,z-1},
\end{equation}
где $z$ - количество фреймов для конкретной звуковой записи, $N$ - длина одного фрейма.

После того, как посчитаны мгновенные энергии для каждого фрейма, вычисляются нижнее и верхнее пороговые значения:
\begin{equation}
\begin{aligned}
& I_1 = 0.03 \cdot (MX - MN) + MN \\
& I_2 = 4 \cdot MN \\
& ITL = min(I_1,~I_2)\\
& ITU = 10 \cdot ITL,
\end{aligned}
\end{equation}
где $MN$, $MX$ - минимум и максимум мгновенной энергии среди всех фреймов соответственно, $ITL$, $ITU$ - нижнее и верхнее пороговое значение.

Происходит поиск фрейма, с которого начинается слово, начиная с самого первого фрейма. Фрейм, в котором значение мгновенной энергии превышает $ITL$, предварительно помечается как начало слова. Затем, начиная с этого помеченного фрейма, происходит поиск фрейма, в котором значение мгновенной энергии превышает $ITU$. Если значение мгновенной энергии для какого-то фрейма во время последнего поиска меньше $ITL$, то этот фрейм становится предварительным началом слова. 

Аналогично происходит поиск конца слова в звуковой дорожке, но поиск по фреймам происходит не с начала сигнала, а с конца.

После этого этапа имеются два предварительно помеченных фрейма $m_{begin}, m_{end}$  - начало и конец слова в звуковом файле соответственно.

Функция мгновенной энергии, определённая формулой \eqref{eq:instant_energy}, хорошо справляется с отделением звонких звуков от тишины. Но глухие она отделяет плохо. Поэтому используется вторая характеристика для доопределения начала и конца слова - число переходов через ноль. Это количество таких случаев, когда соседние значения амплитуд имеют противоположные знаки. Определяется формулой:
\begin{equation}
	Z_k = \dfrac{1}{2} \sum_{m=1}^{N-1} |sgn(x_{k_{m-1}}) - sgn(x_{k_m})|,~k=\overline{0,z-1},
\end{equation}
где $z$ - количество фреймов для конкретной звуковой записи, $N$ - длина одного фрейма.

Подразумевается, что первые 100 мс звуковой записи - это тишина, и речь начинается позднее.

Вычисляется среднее значение переходов через ноль в течение первых 100 мс \eqref{eq:izc} и среднее квадратическое отклонение количества переходов через ноль в течение первых 100 мс \eqref{eq:deviation}:
\begin{align}
	\label{eq:izc}
	&IZC = \dfrac{1}{z} \sum_{k=0}^{z-1} Z_k \\
	\label{eq:deviation}
	&\sigma_{IZC} = \sqrt{\dfrac{1}{z} \sum_{k=0}^{z-1} (Z_k - IZC)^2},
\end{align}
где $z$ - количество фреймов для конкретной звуковой записи.
Затем вычисляется значение пороговой функции числа переходов через ноль по формуле:
\begin{equation}
	IZCT = min(IF,~IZC + 2 \sigma_{IZC}),
\end{equation}
где $IF$ - фиксированное количество переходов через ноль. В данном случае оно составляет 25 пересечений за 10 мс, то есть $IF=2.5$. 

Далее происходит уточнение точек начала и конца слова в звуковой дорожке. Начиная от фрейма $m_{begin}$ влево происходит поиск фреймов, у которых число переходов через ноль выше порогового значения. Поиск происходит на расстоянии 25 фреймов, так как производится уточнение границ слова. Если пороговое значение было превышено 3 или более раз, то фрейм, где это произошло впервые, помечается как начало слова и обозначается как $r_{begin}$. Если пороговое значение было превышено менее 3-х раз, то метка $m_{begin}$ переобозначается как $r_{begin}$.

Аналогично от фрейма $m_{end}$ происходит поиск вправо для уточнения точки конца слова, которая обозначается как $r_{end}$.

В результате уточнения точек начала и конца слова имеются 2 помеченных фрейма - $r_{begin}, r_{end}$. Сигнал обрезается, и в нем остаётся только речевая команда в виде набора фреймов $[r_{begin}, ... , r_{end}]$. Количество получившихся фреймов обозначается как $u$, а сами фреймы перенумеруются следующим образом: $[r_0, ... , r_{u-1}]$.

\subsection{Выделение речевых признаков}
Для того, чтобы выделить речевые признаки, используется алгоритм мел-частотных кепстральных коэффициентов \cite{MFCC} (далее - MFCC). Он является одним из стандартных подходов к решению поставленной задачи. Состоит MFCC из нескольких шагов:
\begin{enumerate}
	\item Для каждой звукового сигнала проделать шаги:
	\begin{enumerate}
		\item Разбить сигнал на фреймы.
		\item Для каждого фрейма проделать шаги:
		\begin{enumerate}
			\item Получить спектр сигнала.
			\item Составить набор мел-фильтров.
			\item Применить мел-фильтры к спектру сигнала.
			\item Прологарифмировать результат, полученный на предыдущем шаге.
			\item Применить дискретное косинусное преобразование к результату предыдущего шага.
		\end{enumerate}
		\item Объединить MFCC векторы коэффициентов в матрицу.
	\end{enumerate}

	\item Привести матрицы коэффициентов MFCC каждой звуковой дорожки к одной размерности. Для этого дополнить их нулями слева до необходимой длины. Унифицированная размерность матриц выбирается как максимальная среди всех.
\end{enumerate}
 
% Need to translate
Why do we do these things?

We will now go a little more slowly through the steps and explain why each of the steps is necessary.

An audio signal is constantly changing, so to simplify things we assume that on short time scales the audio signal doesn't change much (when we say it doesn't change, we mean statistically i.e. statistically stationary, obviously the samples are constantly changing on even short time scales). This is why we frame the signal into 20-40ms frames. If the frame is much shorter we don't have enough samples to get a reliable spectral estimate, if it is longer the signal changes too much throughout the frame.

The next step is to calculate the power spectrum of each frame. This is motivated by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. Depending on the location in the cochlea that vibrates (which wobbles small hairs), different nerves fire informing the brain that certain frequencies are present. Our periodogram estimate performs a similar job for us, identifying which frequencies are present in the frame.

The periodogram spectral estimate still contains a lot of information not required for Automatic Speech Recognition (ASR). In particular the cochlea can not discern the difference between two closely spaced frequencies. This effect becomes more pronounced as the frequencies increase. For this reason we take clumps of periodogram bins and sum them up to get an idea of how much energy exists in various frequency regions. This is performed by our Mel filterbank: the first filter is very narrow and gives an indication of how much energy exists near 0 Hertz. As the frequencies get higher our filters get wider as we become less concerned about variations. We are only interested in roughly how much energy occurs at each spot. The Mel scale tells us exactly how to space our filterbanks and how wide to make them. See below for how to calculate the spacing.

Once we have the filterbank energies, we take the logarithm of them. This is also motivated by human hearing: we don't hear loudness on a linear scale. Generally to double the percieved volume of a sound we need to put 8 times as much energy into it. This means that large variations in energy may not sound all that different if the sound is loud to begin with. This compression operation makes our features match more closely what humans actually hear. Why the logarithm and not a cube root? The logarithm allows us to use cepstral mean subtraction, which is a channel normalisation technique.

The final step is to compute the DCT of the log filterbank energies. There are 2 main reasons this is performed. Because our filterbanks are all overlapping, the filterbank energies are quite correlated with each other. The DCT decorrelates the energies which means diagonal covariance matrices can be used to model the features in e.g. a HMM classifier. But notice that only 12 of the 26 DCT coefficients are kept. This is because the higher DCT coefficients represent fast changes in the filterbank energies and it turns out that these fast changes actually degrade ASR performance, so we get a small improvement by dropping them.  
% Need to translate
 
Так как на выходе алгоритма выделения начальной и конечной точек слова получается набор фреймов, то шаг разбиения сигнала на фреймы опускается.

\subsubsection{Получение спектра сигнала}
Спектр сигнала $S_{k_m}$ в $k$-ом фрейме - результат дискретного преобразования Фурье:
\begin{equation}
	S_{k_m} = \sum_{n=0}^{N-1} x_{k_n} \cdot e^{\dfrac{-2\pi i}{N}mn},~m=\overline{0,N-1},~k=\overline{0,u-1},
\end{equation}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $N$ - длина одного фрейма, $i$ - мнимая единица.

\subsubsection{Мел-шкала и расчёт мел-фильтров}
Мел - психофизическая единица высоты звука. Она описывает значимость конкретной частоты в человеческом восприятии. 
Популярные формулы для перевода Герц в 
мел и обратно описаны в книге \cite{Mel} на стр. 150:
\begin{align}
	\label{eq:mel}
	&mel(hz) = 1127 \cdot ln(1 + \dfrac{hz}{700})\\
	\label{eq:hz}
	&hz(mel) = 700 \cdot (e^{\dfrac{mel}{1127}}-1)
\end{align}

На рисунке \ref{fig:hz_mel} изображено сравнение шкал мел и Гц.

\begin{figure}[H]
	\[\includegraphics[scale=0.3]{hz_mel.png}\]
	\caption{Сравнение шкал мел и Гц}
	\label{fig:hz_mel}
\end{figure}


Составляются треугольные мел-фильтры в виде оконной функции:
\begin{equation}
	H(v,~b)=
	\begin{cases}
		0, 										     & \Phantom \phantom{-}    b < f(v)\\
		\dfrac{b-f(v)}{f(v+1)-f(v)},   & \Phantom f(v) \leq b < f(v+1)\\
		\dfrac{f(v+2)-b}{f(v+2)-f(v+1)}, & \Phantom f(v+1) \leq b < f(v+2)\\
		0,                                           & \Phantom \phantom{-}   f(v+2) < b\\
	\end{cases}
,
\end{equation}
для которой $f$ определяется как:
\begin{equation}
	f(a)=\dfrac{N}{w} hz(mel(f_{min})+a \dfrac{mel(f_{max})-mel(f_{min})}{Q+1}),
\end{equation}
где $mel$ и $hz$ - функции, определённые формулами \eqref{eq:mel} и \eqref{eq:hz} соответственно, $w$ - частота дискретизации звуковой дорожки, $f_{min},~f_{max}$ - нижний и верхний пороги частотного диапазона соответственно, $N$ - длина одного фрейма, $Q$ - количество мел-фильтров. Параметр Q обычно выбирается в диапазоне 20-40 (26 является стандартом).

На рисунке \ref{fig:filters} в качестве примера изображена оконная функция для звуковой дорожки с параметрами $w=44100$ Гц, $f_{min}=0$ Гц, $f_{max}=22050$ Гц, $N=1024$, $Q=13$.

\begin{figure}[H]
	\[\includegraphics[scale=0.3]{filters.png}\]
	\caption{Оконная функция}
	\label{fig:filters}
\end{figure}

\subsubsection{Получение кепстра сигнала}
Кепстр -  определяется в виде прямого преобразования Фурье от логарифма спектра мощности сигнала:
\begin{equation}
	C(x(t))=F(ln[F(x[t])]),
\end{equation}
где $x(t)$ - входной сигнал, $F$ - функция прямого преобразования Фурье.

Кепстр совместно с мел-фильтрами используется для выделения речевых признаков. Эта та часть звукового сигнала, которая была образована при помощи голосового тракта человека. В книге \cite{CeptrumExplanation} можно найти описание того, как это происходит на стр. 367-380. 

Выше описан кепстр для непрерывного сигнала. В случае с дискретным набором амплитуд в сигнале и учитывая мел-фильтры, $ln[F(x[t])$ записывается как:
\begin{equation}
	W_{k_q} = ln(\sum_{m=0}^{N-1} |S_{k_m}|^2 \cdot H(q,m)),~q=\overline{0,Q-1},~k=\overline{0,u-1},
\end{equation}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $Q$ - выбранное количество мел-фильтров.

В MFCC вместо прямого преобразования Фурье над логарифмом спектра мощности сигнала используется дискретное косинусное преобразование как его частный случай прямого преобразования Фурье. Получение итоговых коэффициентов MFCC происходит по формуле:
\begin{equation}
	c_{k_n} = ln(\sum_{m=0}^{Q-1} W_{k_m} cos(\dfrac{\pi}{Q} (m+\dfrac{1}{2})n)),~n=\overline{0,d-1},d \le Q,~k=\overline{0,u-1},
\end{equation}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $Q$ - выбранное количество мел-фильтров, $d$ - выбранное количество коэффициентов MFCC (обычно $d=12$), $\pi$ - математическая константа.

Таким образом, строится матрица для каждой звуковой дорожки следующего вида:

\begin{equation*}
	C_{u \times d} = \left(
	\begin{array}{cccc}
		c_{00} & c_{01} & \ldots & c_{0(d-1)}\\
		c_{10} &  c_{11} & \ldots & c_{1(d-1)}\\
		\vdots & \vdots & \ddots & \vdots\\
		c_{(u-1)0} & c_{(u-1)1} & \ldots & c_{(u-1)(d-1)}
	\end{array}
	\right)
\end{equation*}
где $u$ - количество фреймов, получившееся после выделения начальной и конечной точек слова, $d$ - выбранное количество коэффициентов MFCC.

\label{par:unify_coeffs}
\subsubsection{Приведение данных к одной размерности}
Среди всех значений $u$ существует максимальное $u_{max}$. Для каждой матрицы, соответствующей определённой звуковой дорожке, проделаем следующее:
\begin{itemize}[leftmargin=2cm]
	\item если $u < u_{max}$, то соответствующая матрица $C$ дополняется нулями слева:
	\begin{equation*}
		C_{u_{max} \times d} = \left(
		\begin{array}{ccccccc}
			0 & \ldots & 0 & c_{00} & c_{01} & \ldots & c_{0(d-1)}\\
			0 & \ldots & 0 & c_{10} &  c_{11} & \ldots & c_{1(d-1)}\\
			\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
			0 & \ldots & 0 & c_{(u-1)0} & c_{(u-1)1} & \ldots & c_{(u-1)(d-1)}
		\end{array}
		\right)
	\end{equation*}
	\item если $u = u_{max}$, то матрица $C$ остаётся без изменений.
\end{itemize}

Таким образом, все матрицы приводятся к одной размерности $u_{max} \times d$.

\subsection{Распознавание речевых команд}
Распознавание речевых команд происходит при помощи применения технологии нейронных сетей. В данной работе рассматриваются два типа нейронных сетей: многослойный персептрон и свёрточная сеть. Производится сравнение производительности этих двух типов при разных параметрах обучения.
\subsubsection{Многослойный персептрон}
Этот вид нейронной сети описан в третьей части книги Фрэнка Розенблатта \cite{PerceptronBook}, который первый предложил модель персептрона. Она состоит их входного слоя, полносвязных слоев и выходного слоя.
\subsubsection{Свёрточная нейронная сеть}
Данный тип нейронной сети был предложен Яном ЛеКуном \cite{CNN}. Это многослойная сеть, позволяющая обеспечить устойчивость распознавания к инвариантным изменениям данных за счёт общих весов и локального рецептивного поля.

Входной слой сети состоит из одной плоскости. Его размерность совпадает с размерностью входных данных. 

Последующие слои - свёрточные. Каждый свёрточный слой состоит из нескольких плоскостей нейронов, которые известны как карты признаков. Каждый нейрон в свёрточном слое соединён с небольшой областью предыдущего слоя. В этом заключается принцип локального рецептивного поля.

После каждого свёрточного слоя, который получил локальные признаки, стоит пулинг слой. Его задача состоит в понижении размерности данных.

После всех свёрточных слоёв следует выпрямляющий слой, который преобразует данные к вектору. 

Далее следуют полносвязные слои. Иногда добавляется дропаут слой, который отключает некоторые случайные нейроны в процессе обучения. Это позволяет бороться с переобучением сети.

Выходной слой имеет размерность требуемых выходных данных. 

\subsubsection{Входные и выходные данные модели}
Входной тензор модели - матрица MFCC коэффициентов для соответствующей команды. Его размерность - $u_{max} \times d$. Для составленного датасета $u_max=400, d=13$.

Выходной тензор имеет размерность $g \times 1$, где $g$ - количество возможных команд для распознавания. Для составленного датасета $g=11$. Скалярная метка распознанной команды соответствует индексу максимального элемента выходного тензора. Индексация выходном тензоре начинается с 0.
\subsubsection{Архитектура нейронной сети}
Архитектура свёрточной нейронной сети приведена на рисунке \ref{fig:cnn_model}. Как видно из рисунка, сеть включает в себя:
\begin{itemize}[leftmargin=2cm]
	\item Входной слой InputLayer (размерность входных данных - $400 \times 13$)
	\item Слой свёртки Conv2D (32 нейрона, размерность ядра сверки - $5 \times 5$, функция активации - ReLu)
	\item Слой пулинга AveragePooling2D (размерность пула - $2 \times 2$)
	\item Слой свёртки Conv2D (64 нейрона, размерность ядра сверки - $5 \times 5$, функция активации - ReLu)
	\item Слой пулинга AveragePooling2D (размерность пула - $2 \times 2$)
	\item Выпрямляющий слой Flatten
	\item Полносвязный слой Dense (128 нейронов, функция активации - ReLu)
	\item Слой дропаута Dropout (процент исключения случайных нейронов от общего числа в слое - 30\%)
	\item Выходной полносвязный слой Dense (11 нейронов, функция активации - Softmax)
\end{itemize}

\begin{figure}[H]
	\[\includegraphics[scale=0.4]{cnn_model.png}\]
	\caption{Структура свёрточной нейронной сети}
	\label{fig:cnn_model}
\end{figure}

Архитектура свёрточной нейронной сети приведена на рисунке \ref{fig:cnn_model}. Как видно из рисунка, сеть включает в себя:
\begin{itemize}[leftmargin=2cm]
	\item Входной слой InputLayer (размерность входных данных - $400 \times 13$)
	\item Выпрямляющий слой Flatten
	\item Полносвязный слой Dense (256 нейронов, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Полносвязный слой Dense (128 нейронов, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Полносвязный слой Dense (128 нейронов, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Полносвязный слой Dense (64 нейрона, функция активации - ReLu, регуляризатор ядра - L2 c параметром $\lambda = 0.00001$)
	\item Выходной полносвязный слой Dense(11 нейронов, функция активации - Softmax)
\end{itemize}

\begin{figure}[H]
	\[\includegraphics[scale=0.4]{mlp_model.png}\]
	\caption{Структура многослойного персептрона}
	\label{fig:cnn_model}
\end{figure}

\subsection{Программная реализация}
Построение программного интерфейса для блоков предобработки и распознавания команд было произведено при помощи языка программирования Python 3.8.

Блок предобработки был реализован при помощи библиотек numpy, scipy. 

Блок распознавания был реализован при помощи библиотек jupyterlab, Keras, numpy. Для анализа результатов обучения нейронных сетей были использованы библиотеки pandas, matplotlib, seaborn.

Программный комплекс разделен на две основные части:
\begin{itemize}[leftmargin=2cm]
	\item Корневой файл <<preprocessing.py>>, в котором реализована предобработка. Весь вспомогательный функционал вынесен в отдельный модуль <<helpers>>.
	\item Корневой файл <<boxy.ipynb>>, в котором реализовано обучение и тестирование нейронных сетей. Весь вспомогательный функционал также вынесен в отдельный модуль <<helpers>>.
\end{itemize}

В корневой директории <<recorded\_audio>> хранятся записанные шестью разными дикторами звуковые файлы, содержащие команды. Каждый файл содержит в своём названии индекс речевой команды, которая в нём записана.

В корневую директорию <<data>>, в процессе предобработки файлов из директории <<recorded\_audio>>, сохраняются файлы:
\begin{itemize}[leftmargin=2cm]
	\item <<speaker\{$i$\}\_data.npy>>\footnotemark 
	\item <<speaker\{$i$\}\_labels.npy>>\footnotemark[\value{footnote}],
\end{itemize}
где  $i=\overline{1,6}$ - номер диктора. Их описание приводится в разделе \ref{par:saving_preprocessed_data}.

В корневой директории <<logs>> в процессе предобработки данных создаётся файл <<log.log>>, в который записывается краткий отчёт об обработке каждой звукового файла.

В корневой директории <<model\_checkpoints>> в процессе обучения нейронной сети создаются дочерние директории <<experiment\{$i$\}>>\footnotemark[\value{footnote}], $i=\overline{1,3}$ - номер эксперимента. В этой дочерней директории создаётся своя дочерняя директория <<{nn\_type}>>. Параметр nn\_type принимает  значения <<cnn>> при обучении свёрточной сети и <<mlp>> при обучении многослойного персептрона. В директорию <<{nn\_type}>> сохраняются веса сети с наилучшей точностью распознавания на валидационных данных.

\footnotetext[1]{Фигурные скобки не являются частью имени файла или директории.}

Для создания датасета было разработано веб-приложение при помощи языков Javascript, HTML, CSS и фреймворка NodeJS. Этот сервис позволяет записывать речевые команды и сохранять их в нужном для программы предобработки wav формате с правильным названием. Это позволило в короткие сроки записать необходимое количество дикторов и уменьшило скорость записи в несколько раз, так как в обычных программах записи звука очень много времени уходит на сохранение с правильными параметрами звуковой дорожки. Код веб-приложения доступен в приложении.

\subsection{Описание датасета}
Составленный датасет состоит из 11 команд, записанных шестью дикторами. Каждый диктор работал с командами : <<back>>, <<down>>, <<menu>>, <<off>>, <<on>>, <<open>>, <<play>>, <<power>>, <<stop>>, <<up>>, <<volume>>. В таблице \ref{table:dataset} указаны типы голосов дикторов и данные количестве записанных команд.
\begin{table}[H]
\begin{tabular}[c]{ | p{1.8cm} | p{2.5cm} | p{6cm} | p{4cm} | }
	\hline
	Диктор & Тип голоса & Кол-во звук. дорожек на каждую команду & Сумм. кол-во звук. дорожек  \\ \hline
	speaker1 & Мужской & 50 & 550 \\
	speaker2 & Мужской & 40 & 440 \\
	speaker3 & Мужской & 40 & 440 \\
	speaker4 & Мужской & 40 & 440 \\
	speaker5 & Мужской & 50 & 550 \\
	speaker6 & Женский & 50 & 550 \\ \hline
	
\end{tabular}
\caption{\label{table:dataset}Типы дикторов и данные количестве записанных команд}
\end{table}

Датасет предварительно разделяется на тренировочную и тестовую части. На тренировочную часть отводится 70\% данных каждого диктора, на тестовую часть - 30\%.

\label{par:saving_preprocessed_data}
Матрицы коэффициентов MFCC, полученных на этапе \ref{par:unify_coeffs} объединяются в массив и записываются в файл <<speaker\{$i$\}\_data.npy>>\footnotemark[\value{footnote}] \space в виде numpy массива. В файл <<speaker\{$i$\}\_labels.npy>>\footnotemark[\value{footnote}] записываются скалярные метки команд в виде массива, в том же порядке, что и соответствующие им матрицы коэффициентов MFCC. Здесь $i=\overline{1,6}$  - номер диктора. Соответствие произнесённых диктором команд и их скалярных меток представлено в таблице \ref{table:commands}.

\footnotetext[1]{Фигурные скобки не являются частью имени файла или директории.}

\begin{table}[H]
	\small
	\begin{tabular}[c]{ | l | l | l | l | l | l | l | l | l | l | l | l |}
		\hline
		Команда	 &  back & down & menu & off & on & open & play & power & stop & up & volume \\ \hline
		Скалярная метка & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
		
	\end{tabular}
	\caption{\label{table:commands}Типы дикторов и данные количестве записанных команд}
\end{table}